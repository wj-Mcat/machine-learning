# ä½¿ç”¨CNNï¼ŒRNNå’ŒHAN è¿›è¡Œæ–‡æœ¬åˆ†ç±»

![](../../assert/imgs/text-classification-what-it-is2.png)

ğŸ”¥ å¦‚é¢˜ï¼Œæœ¬æ–‡å°†è¦ä»‹ç»ä¸‰ç§æ¨¡å‹ï¼š

-  **Convolutional Neural Network (CNN)** 
-  **Recurrent Neural Network (RNN)** 
-  **Hierarchical Attention Network (HAN)** 

## ä»‹ç»

æ–‡æœ¬åˆ†ç±»æ˜¯è‡ªç„¶è¯­è¨€å¤„ç†å’Œç›‘ç£å­¦ä¹ é¢†åŸŸä¸€ä¸ªéå¸¸çƒ­é—¨çš„å­ä»»åŠ¡ï¼Œå¾ˆå¤šæ–°æ‰‹çš„å­¦ä¹ ä¹Ÿéƒ½æ˜¯ä»æ–‡æœ¬åˆ†æ¥å¼€å§‹çš„ã€‚é‚£ä¹ˆæ–‡æœ¬åˆ†ç±»æ˜¯å•¥ï¼ŸğŸ™ˆ

ç±»ä¼¼äºåˆ¤å®šä¸€åˆ™æ–°é—»æ˜¯å¦æ˜¯åƒåœ¾æ–°é—»ï¼Œé€šå¸¸æ­¤ç±»æ•°æ®åªéœ€ä¸¤ä¸ªå­—æ®µï¼Œæ¯”å¦‚ï¼š`review`ï¼Œ`label`ç­‰ã€‚æˆ‘ä»¬éœ€è¦åšçš„å°±æ˜¯æ ¹æ®å¸¦æ ‡ç­¾çš„æ–‡æœ¬æ•°æ®ï¼Œè®­ç»ƒå‡ºä¸€ä¸ªåˆ¤åˆ«å™¨ï¼Œç”¨äºè¯†åˆ«å…¶ç±»å‹çš„æ­£è´Ÿã€‚å¸¸è§çš„æ–‡æœ¬åˆ†ç±»ä»»åŠ¡æœ‰ï¼š

- æ–‡æœ¬æƒ…æ„Ÿåˆ†ç±»ï¼ˆå–œæ€’å“€ä¹ï¼‰
- åƒåœ¾é‚®ä»¶åˆ¤åˆ«
- ç”¨æˆ·è¯è¯­æ ‡ç­¾ï¼ˆæŸç±³éŸ³å“ä¸­çš„ï¼šæ”¾æ­Œï¼Œé—­å˜´ï¼Œå…³ç¯ç­‰è¯è¯­ï¼‰
- æ–°é—»ç±»åˆ«åˆ†ç±»ï¼ˆå†å²ï¼Œå¨±ä¹ï¼Œæ”¿æ²»ç­‰ï¼‰

å½“ç„¶åº”ç”¨é¢†åŸŸè‚¯å®šä¸æ­¢è¿™äº›ï¼Œåˆ†ç±»å±äºä¸€ä¸ªéå¸¸åŸºç¡€ä¸”é‡è¦çš„åŠŸèƒ½ï¼Œå­¦å¥½ä¸æ˜“ï¼Œä¸”å­¦ä¸”åŠªåŠ›ã€‚

*****

å®‰åˆ©ä¸€æ³¢æ–‡æœ¬åˆ†ç±»ä»£ç ï¼š

- [Text-Classification]( https://github.com/jatana-research/Text-Classification )
- [brightmart-text-classification]( https://github.com/brightmart/text_classification )
- [cnn-text-classification-tf]( https://github.com/dennybritz/cnn-text-classification-tf )
- ...... (ğŸ‘©â€ğŸ’» è‡ªå·±ä¸Šgayhubä¸Šæ‰¾)

*****

## è¯´æ˜

ä¸€ä¸ªå¥å…¨çš„æ–‡æœ¬åˆ†ç±»ä»»åŠ¡æ˜¯éœ€è¦å‡†å¤‡ä»¥ä¸‹ç´ æçš„ï¼š

- **è®­ç»ƒè¯­æ–™** ï¼šæ²¡æœ‰è¿™ä¸ªéƒ½æ˜¯*æ·¡

- **WordEmbedding**ï¼šå¯ä»¥ä½¿ç”¨é¢„è®­ç»ƒå¥½çš„è¯å‘é‡ï¼Œä¹Ÿå¯ä»¥è‡ªå·±è®­ç»ƒè¯å‘é‡

  é¢„è®­ç»ƒå¥½çš„è¯å‘é‡ï¼š 
  - ä¸­æ–‡ï¼šhttps://github.com/Embedding/Chinese-Word-Vectors 
  - è‹±æ–‡ï¼š
    - [glove](https://nlp.stanford.edu/projects/glove/ )
    - [fasttext]( https://fasttext.cc/docs/en/english-vectors.html )
    - [word2vec]( https://github.com/xgli/word2vec-api )

- **æ ‡ç­¾**ï¼šè¿™ä¸ªä¸€èˆ¬æ˜¯å’Œè®­ç»ƒè¯­æ–™æ”¾åœ¨ä¸€èµ·çš„ï¼Œç”±æ ‡ç­¾ç§ç±»çš„æ•°é‡å¯ä»¥å°†æ–‡æœ¬åˆ†ç±»è®¤ä¸ºäºŒåˆ†ç±»å’Œå¤šåˆ†ç±»ä»»åŠ¡

- **æ¨¡å‹**ï¼šç”¨æ¥è®­ç»ƒè¯­æ–™ç”Ÿæˆæœ€ç»ˆçš„åˆ¤åˆ«å™¨



å¥½äº†ï¼Œè¯¥ä»‹ç»çš„åºŸè¯éƒ½è¯´å®Œäº†ï¼Œæ¥ä¸‹æ¥å°±å¼€å§‹ä»‹ç»å…¶ä¸­å¤šç§æ¨¡å‹äº†ã€‚æœ‰ç ”ç©¶èƒŒæ™¯çš„åŒå­¦ï¼Œ**ä¸€å®šè¦çœ‹è®ºæ–‡**ï¼Œ**ä¸€å®šè¦çœ‹è®ºæ–‡**ï¼Œ**ä¸€å®šè¦çœ‹è®ºæ–‡**ï¼Œåœ°å€åœ¨æœ€åº•ä¸‹è‡ªå·±æ‰¾ï¼Œæœ¬æ–‡ä¸­çš„å›¾ä¹Ÿå¤§å¤šæ•°æ¥è‡ªäºåŸè®ºæ–‡ã€‚

## CNN

é¦–å…ˆæˆ‘å‡è®¾å„ä½å¤§ä½¬ä»¬éƒ½äº†è§£CNNçš„åŸºæœ¬åŸç†ï¼Œé‚£å¦‚ä½•å°†CNNåº”ç”¨åœ¨æ–‡æœ¬ä¸Šå‘¢ï¼Ÿåœ¨æ­¤æˆ‘é€šè¿‡[TextCNN]( https://arxiv.org/pdf/1408.5882.pdf )æ¥è®²è§£ï¼Œå‚è€ƒäº†[ What Does a TextCNN Learn? ]( https://arxiv.org/pdf/1801.06287.pdf )è®ºæ–‡æ¥ç»™å¤§å®¶è®²è§£ï¼Œå¸Œæœ›å¤§å®¶èƒ½å¤Ÿå¸æ”¶æ¶ˆåŒ–20%å°±å·²ç»å¾ˆä¸é”™äº†ã€‚

é¦–å…ˆæ¥çœ‹çœ‹å…¶æ•´ä½“æ¶æ„å›¾ï¼š

![TextCNN](../../assert/imgs/textcnn.png)

**å·¦è¾¹**æ˜¯ç”±ä¸¤ä¸ª `static vectors `å’Œ `non-static vectors`ç»„æˆï¼Œä¸¤è€…çš„åŒºåˆ«åœ¨äºè¯å‘é‡èƒ½å¦**å­¦ä¹ **ï¼ˆå¾®è°ƒï¼‰ã€‚è¿™ä¸ªç‰¹æ€§èƒ½å¤Ÿè®©ç½‘ç»œæ‹¥æœ‰ä¸€å®šçš„åŸºç¡€*æ–‡å­—å…³ç³»* çš„ **è®°å¿†**å’Œ**å­¦ä¹ **èƒ½åŠ›ã€‚

> è¯å‘é‡ä¸€èˆ¬åŸºäºå¤§é‡çš„è¯­æ–™åº“å­¦ä¹ è€Œæ¥ï¼Œé‡Œé¢åŒ…å«è¯­æ³•å’Œè¯­ä¹‰çš„ç›¸å…³ä¿¡æ¯ï¼Œè¿™äº›ä¿¡æ¯å¯ä»¥ä½¿ç”¨**ç›¸ä¼¼åº¦**æ¥è¿›è¡Œæè¿°ï¼Œé€šè¿‡**ç»Ÿè®¡æ–¹æ³•**å¾—æ¥ã€‚è€Œè¿™äº›ä»…ä»…åªæ˜¯é€šè¿‡ä¸€å®šçš„ç»Ÿè®¡æ–¹æ³•å¾—æ¥çš„ç‰¹å¾ï¼Œåˆ°åº•èƒ½ä¸èƒ½ç»å¾—èµ·**éªŒè¯**ï¼ˆlosså‡½æ•°çš„éªŒè¯ï¼‰å°±ä¸å¾—è€ŒçŸ¥äº†ï¼Œæ‰€ä»¥æ­¤å¤„çš„å­¦ä¹ èƒ½åŠ›æ˜¯éå¸¸é‡è¦çš„ã€‚
>
> å¦å¤–ï¼Œå¦‚æœå¯¹è¯å‘é‡çš„å­¦ä¹ å¯ä»¥çœ‹çœ‹[è¿™ç¯‡è®ºæ–‡]( https://www.aclweb.org/anthology/P14-1146/ )æˆ–è€…[è¿™ç¯‡åšå®¢]( https://lilianweng.github.io/lil-log/2017/10/15/learning-word-embedding.html )

ä¸­é—´å°±æ˜¯å·ç§¯æ ¸å­¦ä¹ è€Œæ¥çš„ç»“æœï¼Œæ¥ä¸‹æ¥åš`max pooling`æ“ä½œï¼Œé€‰å‡ºä¸€ä¸ª`channel`æœ€å¤§çš„å€¼æœ€ç»ˆæ‹¼æ¥åˆ°ä¸€èµ·ã€‚ç„¶åå†æ¥ä¸ªå…¨è¿æ¥ã€‚ç®€å•çš„ä»£ç å¦‚ä¸‹æ‰€ç¤ºï¼š

```python
sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')
embedded_sequences = embedding_layer(sequence_input)
l_cov1= Conv1D(128, 5, activation='relu')(embedded_sequences)
l_pool1 = MaxPooling1D(5)(l_cov1)
l_cov2 = Conv1D(128, 5, activation='relu')(l_pool1)
l_pool2 = MaxPooling1D(5)(l_cov2)
l_cov3 = Conv1D(128, 5, activation='relu')(l_pool2)
l_pool3 = MaxPooling1D(35)(l_cov3)  # global max pooling
l_flat = Flatten()(l_pool3)
l_dense = Dense(128, activation='relu')(l_flat)
preds = Dense(len(macronum), activation='softmax')(l_dense)
```

æœ€åç”Ÿæˆçš„æ¨¡å‹çš„æ¶æ„å›¾å¦‚ä¸‹æ‰€ç¤ºï¼š

![TextCNNç»“æ„å›¾](../../assert/imgs/text-cnn-structure.png)

ä¸ºå•¥CNNèƒ½å¤Ÿåœ¨æ–‡æœ¬åºåˆ—ä¸­å­¦ä¹ åˆ°åºåˆ—ç‰¹å¾å‘¢ï¼Ÿåé¢æˆ‘ä¹Ÿä¼šå‡ºç›¸å…³çš„åšæ–‡é˜è¿°ã€‚

## RNN

ä½¿ç”¨RNNåŠå…¶æ‰©å±•æ¨¡å‹æ¥å¤„ç†æ–‡æœ¬åºåˆ—æ¨¡å‹æ‰æ˜¯å¤§å®¶è®¤ä¸ºçš„å¸¸è§æ“ä½œï¼Œä¹Ÿè¢«å®è·µè¯æ˜æ˜¯å¤„ç†æ•ˆæœæœ€å¥½çš„æ–¹æ³•ã€‚åœ¨æ­¤RNNçš„åŸºæœ¬æ¦‚å¿µæˆ‘å°±ä¸è¯´äº†ï¼Œä¸ç†Ÿæ‚‰çš„ä¹Ÿå¯ä»¥çœ‹[Colahâ€˜s blog](http://colah.github.io/posts/2015-08-Understanding-LSTMs/)ï¼Œå¦å¤–å¯¹LSTMä¸ç†Ÿæ‚‰çš„å­©ç«¥ä»¬ï¼Œå¯ä»¥çœ‹çœ‹æˆ‘çš„[è¿™ç¯‡åšæ–‡]( https://zhuanlan.zhihu.com/p/82325657 )ï¼Œé‡Œé¢æœ‰è¯¦ç»†è¯´æ˜å…¶ä¸­çš„æ¦‚å¿µã€‚

RNNåœ¨å¤„ç†åºåˆ—æ•°æ®æ–¹é¢èƒ½åŠ›é‚£æ˜¯æ æ çš„ï¼Œå› ä¸ºæœ¬èº«æ¶æ„å°±æ˜¯ä¸ºåºåˆ—è€Œç”Ÿçš„ã€‚åºåˆ—æ•°æ®é¢†åŸŸåŒ…å«ï¼šæ–‡æœ¬ï¼Œæ—¶åºï¼Œè§†é¢‘ï¼ŒDNAæ•°æ®ç­‰ç­‰ã€‚

æ¥ä¸‹æ¥æˆ‘çš„æ¨¡å‹æ¶æ„å¦‚ä¸‹ï¼š

- `Embedding` 

  å¯ä»¥ç›´æ¥ä½¿ç”¨é¢„è®­ç»ƒæ¨¡å‹ï¼Œç§ç±»åœ¨ä¸Šæ–‡

- `LSTM Encoder`

  ä½¿ç”¨LSTMå¯¹wordè¿›è¡Œç®€å•ç¼–ç 

- `Full Connected Network`

  å¯¹è¾“å‡ºç»“æ„è¿›è¡Œå…¨è¿æ¥æ“ä½œï¼Œè¾“å‡ºæœ€ç»ˆåˆ†ç±»

![](../../assert/imgs/HierarchicalNetwork.png)

ç”±ä¸Šå›¾æ‰€è§ï¼Œè¿™æ˜¯ä¸€ä¸ªå¾ˆæ ‡å‡†çš„`Sequence to Sequence`æ¨¡å‹ï¼Œå¯ä»¥åˆ†ä¸ºä»¥ä¸‹å‡ ç‚¹ç†è§£ï¼š

- Encoder

  è¯å‘é‡åºåˆ—å–‚ç»™LSTMä¹‹åï¼Œä¼šç”Ÿæˆä¸€ä¸ªéšè—å±‚å‘é‡ï¼Œä¹Ÿå°±æ˜¯Encoder/LSTMçš„è¾“å‡ºã€‚ä¸Šé¢Encoderä¸­åªæ˜¯åŒ…å«æ¯”è¾ƒç®€å•çš„ä¸€å±‚LSTMï¼Œä¹Ÿå¯ä»¥æ·»åŠ ç¨å¾®å¤æ‚ç‚¹çš„[Attention](https://arxiv.org/abs/1706.03762)æœºåˆ¶ï¼Œä¹Ÿå¯ä»¥æ·»åŠ [å±‚çº§åŸºäºAttentionçš„Sequence2Sequence]( https://arxiv.org/pdf/1506.01057v2.pdf )ç»“æ„ï¼Œç»“æ„å¯ä»¥å¾ˆå¤æ‚ï¼Œä¸€èˆ¬å¤„ç†ä¸‹è¿‡ä¹Ÿæ˜¯å¾ˆå¥½çš„ã€‚ä¸è¿‡å’±ä»¬è¿˜å¾—ä¸€æ­¥ä¸€æ­¥æ¥ã€‚

  Encoderçš„è¾“å‡ºè¢«ç§°ä½œä¸º`Context Vector`ï¼Œè¿™ä¸ªå°±æ˜¯Encoderå¯¹è¾“å…¥è¿™å¥è¯çš„`ç†è§£`ï¼Œç„¶åDecoderå¯¹è¿™å¥è¯è¿›è¡Œç¿»è¯‘ï¼Œè§£ç ã€‚

- Decoder

  ä¸€èˆ¬ç»“æ„ä¸Encoderä¸€è‡´ï¼Œå³ä½¿æœ‰å·®åˆ«ä¹Ÿå·®åˆ«ä¸å¤§ã€‚

  ä½ æ€ä¹ˆç¼–ç ï¼Œæˆ‘å°±æ€ä¹ˆè§£ç ï¼Œä¸ç„¶ä¸¤è€…çš„**è„‘å›è·¯**ä¸ä¸€æ ·ï¼Œç†è§£çš„ç»“æœä¹Ÿå°±ä¸ä¸€æ ·ã€‚

- å…¨è¿æ¥å±‚

  è¿™ä¸ªå°±æ˜¯å¯¹è§£ç åçš„ä¿¡æ¯ç‚¹è¿›è¡Œ**åŠ æƒå¤„ç†**ï¼Œæœ€ç»ˆå¾—åˆ°åˆ†ç±»ç»“æœã€‚

Kerasä»£ç å¦‚ä¸‹æ‰€ç¤ºï¼š

```python
sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')
embedded_sequences = embedding_layer(sequence_input)
l_lstm = Bidirectional(LSTM(100))(embedded_sequences)
preds = Dense(len(macronum), activation='softmax')(l_lstm)
model = Model(sequence_input, preds)
model.compile(loss='categorical_crossentropy',optimizer='rmsprop',  metrics=['acc'])
```

æœ€ç»ˆçš„æ¶æ„å›¾å¦‚ä¸‹æ‰€ç¤ºï¼š

![](../../assert/imgs/lstm-encoder-structure.png)

## HAN 

> å…¨ç§°ï¼šHierarchical Attention Networkï¼Œè¯¦ç»†äº†è§£è¯·çœ‹è¿™ç¯‡[è®ºæ–‡](https://www.cs.cmu.edu/~hovy/papers/16HLT-hierarchical-attention-networks.pdf)ã€‚

è¿™ä¸ªå°èŠ‚ï¼Œæˆ‘å°†è¦ç»™å¤§å®¶ä»‹ç»`å±‚çº§LSTM`ç½‘ç»œã€‚æ¨¡å‹ç»“æ„å¦‚ä¸‹æ‰€ç¤ºï¼š

![](../../assert/imgs/han-structure-model.png)

ä¸Šé¢æˆ‘å·²ç»è®²äº†ä¸¤ç§æ¨¡å‹å›¾ï¼Œå¦‚æœè¿™ä¸ªè¿˜ä¸æ‡‚ï¼Œå¯ä»¥å†å›è¿‡å¤´è¿‡å»çœ‹çœ‹ï¼Œä¹‹åå†å›æ¥çœ‹è¿™ä¸ªåº”è¯¥å°±æ˜ç™½äº†ã€‚

keras ä¼ªä»£ç å¦‚ä¸‹æ‰€ç¤ºï¼š

```python
embedding_layer=Embedding(len(word_index)+1,EMBEDDING_DIM,weights=[embedding_matrix],
input_length=MAX_SENT_LENGTH,trainable=True)
sentence_input = Input(shape=(MAX_SENT_LENGTH,), dtype='int32')
embedded_sequences = embedding_layer(sentence_input)
l_lstm = Bidirectional(LSTM(100))(embedded_sequences)
sentEncoder = Model(sentence_input, l_lstm)

review_input = Input(shape=(MAX_SENTS,MAX_SENT_LENGTH), dtype='int32')
review_encoder = TimeDistributed(sentEncoder)(review_input)
l_lstm_sent = Bidirectional(LSTM(100))(review_encoder)
preds = Dense(len(macronum), activation='softmax')(l_lstm_sent)
model = Model(review_input, preds)
```

ä¸Šè¿°ä»£ç æ¨¡å‹ç»“æ„å¦‚ä¸‹ï¼š

![](../../assert/imgs/han-model.png)

## å‚è€ƒæ–‡ç« åŠè®ºæ–‡ï¼š

> å…¶ä¸­å¤§éƒ¨åˆ†è®ºæ–‡åœ°å€ï¼Œæˆ‘éƒ½æ”¾åœ¨æ–‡ç« ä¸­ï¼Œåœ¨æ­¤å°±ä¸é‡å¤ä¸€ä¸€ç½—åˆ—äº†ã€‚

- [Report on Text Classification using CNN, RNN & HAN]( https://medium.com/jatana/report-on-text-classification-using-cnn-rnn-han-f0e887214d5f )
- [Convolutional Neural Networks for Sentence Classification]( https://arxiv.org/abs/1408.5882 )
- [What Does a TextCNN Learn?]( https://arxiv.org/abs/1801.06287 )
- 